<!DOCTYPE html>
<html>
    <head>
        <link rel="stylesheet" type="text/css" href="style.css" />
        <script src="script.js"></script>
        <title>
            Uncharted Data | Data Subjects
        </title>
        <link rel="apple-touch-icon" sizes="114x114" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
    </head>
    <body>
        <div id="banner">
            <a href="index.html" id="uncharted">Uncharted Data</a>
            <a href="index.html" class="tabs">Home</a>
            <a href="guesser.html" class="tabs">Food Guesser</a>
            <a href="images.html" class="tabs">Images</a>
            <a href="theory.html" class="tabs">Theoretical Background</a>
            <a href="references.html" class="tabs">References</a>
        </div>
        <div id="body">
        </div>
        <div id="body2">
            <h1 class="headline">
                Theoretical Background
            </h1>
            <h2 class="headline">
                Machine learning in our project
            </h2>
            <p class="textBody">
                    Most of us interact with machine learning on a daily basis without thinking about it. In a report by the Royal Society from 2017 they state that “many people already use specific applications of machine learning every day, without being aware of the intelligence under the hood.” (Royal Society 2017: 21). This is for instance in recommender systems, like product suggestions and such services, it is in the way information is organized, like search engines and spam filters etc. Simply put, machine learning is used to detect patterns in large amounts of data (Royal Society 2017: 22-23). <br>
                    This being the purpose, that machine learning models are “simple” grouping mechanisms, and depending on what information the model was given, it gives a response and with it how confident it is, that this would be the right answer or prediction. The argument is that when working with large amounts of data, a computer system is better at finding the answer than a human system is. This raises the question though: what is meant by better? It is of course faster, as a computer can process large amounts of data faster than a human can, but is machine learning more accurate? Less biased? That depends on the data provided and the algorithms used, as computers for machine learning “apply statistical learning techniques to automatically identify patterns in data.” (Yee and Chu). So in our project we teach the computer which food belongs to which member of our group by giving it a labelled dataset, also known as supervised learning. <br>
                    The biggest issue for our project to actually work is that we have a quite small dataset, and that neither of us have been consistent in the collection process. <br>
                    Several factors are at play here. One is that we have made this simply because we can, and not with an expectation of it to work well. Our point comes across even if the computer can’t distinguish between us, based on pictures of our meals. It works as an example of how machine learning works, and how we can use it theoretically. Another important factor for the quality of our dataset is that hunger apparently really affected the collection process, as we all often simply forgot to take a picture of our meals.<br> 
                    <br>
            </p>
            <h2 class="headline">
                Subject and identity
            </h2>
            <p class="textBody">
                Our project is not just about machine learning. The machine learning part is a product from which we can engage and play around with different theories of the subject and identity. We have not invented a new wheel, on the contrary: How many times a week don’t most of us scroll past yet another article from Buzzfeed or the like that says “Answer this question and find out which Kardashian you are!”, or “Take this quiz and find out what kind of cheese you are!” and so on. In our project you can upload a picture of your food and find out whether you are Mads, Jeppe or Sia. There seems to be some kind of constant need to compare ourselves online, and to use these comparisons as representation of identities.<br> 
                <br>
                Ian Hacking has an interesting analysis of the concept which he calls ‘making up people’ (Hacking 1986). It is based on the idea that the way we perceive and understand certain categories of people are created throughout history, reflecting the ideas, interests and needs of that time and place. The example Hacking uses is the idea of the pervert, and how it was created in the 19th century. Not to say that odd people didn’t exist before then, but the very idea of perversion and the perverted person was created. The perverted, as a classification and category, was made. This is interesting, because the idea of ‘making up people’ “affects our very idea of what it is to be an individual.” (Hacking 1986: 161). Because these categories are created in specific contexts, social change therefore creates new categories, which fundamentally “creates new ways for people to be” (ibid.). <br>
                Another interesting actor to look at in relation to our project is Shoshanna Zuboff. According to her, due to the structures that shape our online lives, we have become addicted to likes and followers (Zuboff 2021). She further argues that for adolescents this kind of comparison and self-presentation is not just appropriate, it is natural. It is part of the process of creating our own identities, but is not, in her words, appropriate feelings and behaviours for adults and for the population as a whole. Based on this she claims that it affects our ability to think critically, to self-reflect, and as a consequence it will affect our ability to participate in a democratic society. “We have to be human beings first, not objects. Critical thinkers, not lemmings.” (Zuboff 2021).  She argues that we are turning ourselves into a kind of people that are easier to manipulate. <br>
                Zuboffs ideas draws on the Kantian idea of how we need to dare think for ourselves in order to fully be mature human beings, capable of participating in society (Kant 2017), but it also speaks to Foucaultian ideas of power structures where the modern subject is simultaneously an object of knowledge and of domination. “This form of power applies itself to immediate everyday life which categorizes the individual, marks him by his own individuality, attaches him to his own identity, imposes a law of truth on him which he must recognize and which others have to recognize in him. It is a form of power which makes individuals subjects.” (Foucault 1982: 781). In other words, this form of power is ‘making up people’.<br>
                “Who we are is not only what we did, do, and will do but also what we might have done and may do. Making up people changes the space of possibilities for personhood.” (Hacking 1986: 168). If we as a society becomes addicted to likes and followers as representations of identities, and if we lose the ability to critically engage with the structures that are forming our lives, then “we are losing ownership over the meaning of the categories that constitute our identities'' (Cheney-Lippold 2011: 178). So in relation to our project, it is an attempt to critically engage in these discussions of identities, subjects and data.<br>
                <br>
            </p>
            <h2 class="headline">
                Capitalistic implications
            </h2>
            <p class="textBody">
                Though this project cannot directly be said to be surveillance, one can still gain insights towards other similar forms where machinelearning is deployed. Our project is in close relation to what Jacobsen (2020) describes, where he examines Apple Memories. He concludes that “algorithms construct and spin narratives of the world”, where he in this particular examples references, that Apple Memories’ grouping of images, creates a narrative, creating meaning only through a pattern, not by interpreting nor making sense of it, which “continues to make inroads into social processes”.<br>
                This reflects nicely what Clarke (2019) describes as the feedback given by one’s “digital persona”, which is an abstraction through the cumulative exploitation of the user’s data. “The term ‘exploitation’ is subject to interpretations both positive – as in the extraction of advantage from one’s own assets – and negative – as in unfair utilisation of something belonging to someone else. Both senses of the word are applicable” (Clarke, 2019). This distinction is important to remember, since both discourses are valid.<br>
                There is however an issue in relation to the opacity of these systems. As Lynch (2020) notes, “The remainder of the population [those who are not leaders of the “digital revolution” edited] is positioned as “users” or consumers, the grateful beneficiaries of innovation anxiously awaiting the next “big thing,” whose personal data feed new forms of algorithmic governmentality and control.<br>
                This generates an issue, when the users do not necessarily know either how or why the algorithmic power shapes and narrates their lives, creating an obfuscation of the reality itself.<br>
                With some branches of Computer Science hailing AI, big data, and machine learning “the end of theory” (Anderson, 2008), one could fear an impending corpocracy (Clarke, 2019). This would not only lay claim to science and data, but would lay claim to the narration of life itself, as described in the past sections.<br>
                With companies competing for the user’s data, one could ask the question what the cost is. Though there are positive effects of the systematic data gathering, some would argue that the continuous behavioral value reinvestment cycle (Clarke, 2019), creates a future defined by the past (Cheney-Lippold, 2017) through instrumentarianism (Zuboff, 2019). This forces users, or human natural resources (Zuboff, 2015) or prosumers (Fuchs, 2012) as other would call them, into a Faustian Pact (Zuboff, 2015), where the corpocracy forces users to give up their data, and hence the ability to freely narrate their lives for material goods.<br>
                In order to relate this to the project, we can give an example; while testing some of the food one of the group members consumed, matched poorly with what the machine learning model’s abstraction of said persons datafied food. Shopping is at this time and age highly digitalized, which is evident with for instance Facebook seeming like a “big shopping mall without exit” (Fuchs, 2012). The suggestions one receives are highly influenced by not only one’s own data, but also the persons the algorithm sees in relation to that person. This meaning, that were one to solely use algorithmic based shopping for said person, this person would likely not only get much of the same food, but also food with only fit the dataset that has been gathered by said person.<br>
                This example exemplifies that if one does not have the full picture (all the data), no one, even an algorithm, cannot predict perfectly whose food it is, nor what a person would like or feel. This is the core problem that big data was said to solve since n=all, hence it gaining access to all the data. The issue is that much is left out of the datafied actions, shaped and narrated by the mediation, which creates an incomplete picture, and hence wrong predictions.<br>
                Making algorithms naively (and dataistic) put people into boxes through their generalized social capital (Schwarz, 2019) will move the power and agency from users into the unknown that is packed away neatly into a box called “algorithms”. This does not only affect uses, as Clarke (2019) points out, as the digital surveillance economy threatens societies and polities, by shifting the power from the public sector into the private one, which he points out is largely free of ethical constraints. Question becomes how much power one should give institutions largely driven by capital income, which would in these days both include money as well as data, which can largely be interpreted as holding the same, if not more value (and accumulative value) than money (Sadowski, 2019).<br>
            </p>
        </div>
    </body>
</html>